\section{Introduction}


Large Language Models (LLMs) have shown remarkable improvements in reasoning-intensive tasks by leveraging \emph{chain-of-thought} (CoT) prompting, which encourages models to explicitly generate intermediate reasoning steps before producing a final answer. Recently, the \emph{Self-Rewarded Training} (SRT) framework~\cite{shafayat2025largereasoningmodelsselftrain} proposed scaling this capability through self-improvement: models repeatedly generate multiple reasoning traces for a question, identify the most self-consistent answer, and reinforce those outputs without human supervision. While SRT accelerates progress in early stages, it eventually \emph{collapses}—the model begins to optimize for internal agreement rather than factual correctness, confidently producing coherent but invalid answers. This exposes a deeper issue: SRT lacks a mechanism to verify whether a model’s reasoning remains \emph{faithful} to its final answer.

In this project, we address this gap by measuring the faithfulness of reasoning in self-trained LLMs. Specifically, we investigate whether a model’s chain-of-thought genuinely \emph{causes} its final answer, or whether it merely serves as a decorative explanation generated post hoc. To test this, we introduce a two-model evaluation setup. Given a question, \textbf{Model~A} (Qwen) produces a reasoning chain and a final answer. We then remove the answer and ask \textbf{Model~B} (LLaMA) to infer the answer using only Model~A’s reasoning. If Model~B can reliably reconstruct Model~A’s answer, the reasoning is likely faithful; if not, the CoT no longer grounds the decision. In addition, we apply classical \emph{faithfulness interventions}—such as truncating or perturbing reasoning—to quantify how sensitive answers are to these edits, following the methodology of~\cite{lanham2023measuringfaithfulnesschainofthoughtreasoning}.

The scope of our project is threefold: (1) to develop an evaluation pipeline for CoT faithfulness, (2) to quantitatively analyze how well a model’s reasoning supports its own conclusions, and (3) to identify when self-training begins to drift toward unfaithful reasoning. We conduct experiments on the DAPO-Math-17k dataset, which consists of math word problems requiring multi-step reasoning, allowing us to isolate genuine causal reasoning from pattern completion.

Placed within the broader timeline of the field, this work builds upon SRT’s promise of autonomous model improvement while connecting it to the emerging literature on reasoning faithfulness. Whereas prior studies such as~\cite{lanham2023measuringfaithfulnesschainofthoughtreasoning} focus on static models, we extend these ideas to dynamic, self-training settings where the risk of reasoning collapse is highest. Our results aim to offer a principled diagnostic for determining when self-improvement remains productive—and when it begins to diverge from true reasoning.

