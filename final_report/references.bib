@misc{shafayat2025largereasoningmodelsselftrain,
      title={Can Large Reasoning Models Self-Train?}, 
      author={Sheikh Shafayat and Fahim Tajwar and Ruslan Salakhutdinov and Jeff Schneider and Andrea Zanette},
      year={2025},
      eprint={2505.21444},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2505.21444}, 
}

@misc{lanham2023measuringfaithfulnesschainofthoughtreasoning,
      title={Measuring Faithfulness in Chain-of-Thought Reasoning}, 
      author={Tamera Lanham and Anna Chen and Ansh Radhakrishnan and Benoit Steiner and Carson Denison and Danny Hernandez and Dustin Li and Esin Durmus and Evan Hubinger and Jackson Kernion and Kamilė Lukošiūtė and Karina Nguyen and Newton Cheng and Nicholas Joseph and Nicholas Schiefer and Oliver Rausch and Robin Larson and Sam McCandlish and Sandipan Kundu and Saurav Kadavath and Shannon Yang and Thomas Henighan and Timothy Maxwell and Timothy Telleen-Lawton and Tristan Hume and Zac Hatfield-Dodds and Jared Kaplan and Jan Brauner and Samuel R. Bowman and Ethan Perez},
      year={2023},
      eprint={2307.13702},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2307.13702}, 
}





@inproceedings{wei2022chain,
  title        = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author       = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed H. and Le, Quoc V. and Zhou, Denny},
  booktitle    = {Advances in Neural Information Processing Systems 35 (NeurIPS 2022)},
  year         = {2022},
  url          = {https://arxiv.org/abs/2201.11903},
  note         = {arXiv preprint arXiv:2201.11903}
}




@techreport{openai_o1_systemcard_2024,
  author = {OpenAI},
  title = {OpenAI o1 System Card},
  institution = {OpenAI},
  year = {2024},
  note = {Dec 5 2024 version},
  url = {https://cdn.openai.com/o1-system-card-20240917.pdf}
}

@article{baker2025monitoring,
  author = {Bowen Baker and Joost Huizinga and Leo Gao and Zehao Dou and Melody Y. Guan and Aleksander Mądry and Wojciech Zaremba and Jakub Pachocki and David Farhi},
  title = {Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation},
  journal = {arXiv preprint},
  volume = {arXiv:2503.11926},
  year = {2025},
  url = {https://arxiv.org/abs/2503.11926}
}

@article{korbak2025cot,
  author = {Tomek Korbak and Mikita Balesni and Elizabeth Barnes and Yoshua Bengio and Joe Benton and Joseph Bloom and Mark Chen and Alan Cooney and Allan Dafoe and Anca Dragan and Scott Emmons and Owain Evans and David Farhi and Ryan Greenblatt and Dan Hendrycks and Marius Hobbhahn and Evan Hubinger and Geoff Irving and Daniel Kokotajlo and Victoria Krakovna and Shane Legg and David Lindner and David Luan and Aleksander Mądry and Julian Michael and Neel Nanda and Dave Orr and Jakub Pachocki and Ethan Perez and Mary Phuong and Fabien Roger and Joshua Saxe and Buck Shlegeris},
  title = {Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety},
  journal = {arXiv preprint},
  volume = {arXiv:2507.11473},
  year = {2025},
  url = {https://arxiv.org/abs/2507.11473}
}

@article{darlow2025continuous,
  author = {Luke Darlow and Ciaran Regan and Sebastian Risi and Jeffrey Seely and Llion Jones},
  title = {Continuous Thought Machines},
  journal = {arXiv preprint},
  volume = {arXiv:2505.05522},
  year = {2025},
  url = {https://arxiv.org/abs/2505.05522}
}

@article{anthropic2025reasoning_models,
  author = {Anthropic},
  title = {Reasoning Models Don’t Always Say What They Think},
  year = {2025},
  url = {https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf}
}